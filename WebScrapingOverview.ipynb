{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Web Scraping and Text Classification of Questions Asked on Stack Exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our web scraping functions defined in the SEData package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SEData.data import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate a list of question URLs from Stack Exchange's current \"Hot Network Questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_links = populate_question_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use the get_text function to return a tuple of the SE category and question text corresponding to any SE question URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('academia',\n",
       " ' Im writing a mathematical paper In it I use a lemma The lemma is not hard to prove and I have verified it myself The proof is too tedious to include in the paper so I want to just include a citation I found a paper that includes the result However that paper does not actually include a proof I cannot find any other place where this lemma appears I see three options  State the lemma without proof or citation State the lemma without proof but cite the paper that states the lemma without proof or citation Provide a proof of the lemma  Which is most appropriate Option  is easiest but might annoy some readers who dont believe me Option  seems like a cop out Option  is safest but I dont think its necessary as the proof is really just a long and boring calculation ADDED To be clear the lemma is basically an integral The proof consists of splitting up the domain of integration to remove absolute values evaluating each of the parts easy enough for symbolic integration packages like mathematica and then joining them back up This is obvious but messy because the expressions are quite long My writeup is two pages Maybe a better way to phrase my question The result is trivial  I think so the authors of the other paper think so and the journal they published in thinks so Should I still provide a citation Is it misleading to cite the other paper without clarifying that it doesnt provide a proof ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(question_links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can create a small corpus of 50 questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [get_text(link) for link in question_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data.dropna(axis = 0, how = 'any', inplace = True) #Drop NAs in place. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAs occur for a given url if there is no question text (if the question was removed by moderators) or if the page hasn't been created yet (if the ID is higher than the ID of the most recently asked quesiton within a given SE category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = data[0]\n",
    "corpus = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Im writing a mathematical paper In it I use a...\n",
      "1     I am a childless female something who occasio...\n",
      "2     Background I have been living with my boyfrie...\n",
      "3     I would like to know about the time format wh...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can create a sparse matrix of Term Frequency, Inverse Document Frequency scores for each word in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english', max_features = 500) #Limit number of features at 500 words with highest Tfidf score\n",
    "sparse_matrix = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolute',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'address',\n",
       " 'adult',\n",
       " 'afraid',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'alloys',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'approx',\n",
       " 'argument',\n",
       " 'array',\n",
       " 'ascending',\n",
       " 'ascent',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assume',\n",
       " 'avoid',\n",
       " 'away',\n",
       " 'ax',\n",
       " 'background',\n",
       " 'based',\n",
       " 'basevalue',\n",
       " 'basically',\n",
       " 'battle',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bismuth',\n",
       " 'bit',\n",
       " 'block',\n",
       " 'body',\n",
       " 'bounce',\n",
       " 'brand',\n",
       " 'buy',\n",
       " 'car',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cast',\n",
       " 'cdot',\n",
       " 'center',\n",
       " 'certain',\n",
       " 'change',\n",
       " 'characters',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chores',\n",
       " 'citation',\n",
       " 'class',\n",
       " 'classoption',\n",
       " 'clear',\n",
       " 'closed',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comment',\n",
       " 'common',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'component',\n",
       " 'components',\n",
       " 'consequences',\n",
       " 'constant',\n",
       " 'contentaddclassoptionno',\n",
       " 'cooking',\n",
       " 'countries',\n",
       " 'course',\n",
       " 'cut',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'decelerate',\n",
       " 'decided',\n",
       " 'decimal',\n",
       " 'delta',\n",
       " 'depends',\n",
       " 'descending',\n",
       " 'details',\n",
       " 'diagram',\n",
       " 'did',\n",
       " 'different',\n",
       " 'direction',\n",
       " 'discussion',\n",
       " 'docker',\n",
       " 'does',\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'draw',\n",
       " 'dynamically',\n",
       " 'earlier',\n",
       " 'eat',\n",
       " 'edit',\n",
       " 'eeeee',\n",
       " 'end',\n",
       " 'energy',\n",
       " 'english',\n",
       " 'environment',\n",
       " 'epot',\n",
       " 'equal',\n",
       " 'equivalent',\n",
       " 'escape',\n",
       " 'especially',\n",
       " 'exactly',\n",
       " 'examiner',\n",
       " 'example',\n",
       " 'examples',\n",
       " 'exception',\n",
       " 'executable',\n",
       " 'exert',\n",
       " 'exhausting',\n",
       " 'exist',\n",
       " 'exotic',\n",
       " 'expend',\n",
       " 'explain',\n",
       " 'expressions',\n",
       " 'facing',\n",
       " 'fact',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fall',\n",
       " 'false',\n",
       " 'family',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'fast',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feelings',\n",
       " 'female',\n",
       " 'fh',\n",
       " 'fight',\n",
       " 'file',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'fixed',\n",
       " 'flight',\n",
       " 'following',\n",
       " 'force',\n",
       " 'forever',\n",
       " 'format',\n",
       " 'free',\n",
       " 'friction',\n",
       " 'friends',\n",
       " 'ft',\n",
       " 'fun',\n",
       " 'fx',\n",
       " 'game',\n",
       " 'games',\n",
       " 'genea',\n",
       " 'geneb',\n",
       " 'general',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'government',\n",
       " 'gravity',\n",
       " 'groceries',\n",
       " 'group',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'happen',\n",
       " 'happens',\n",
       " 'having',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'height',\n",
       " 'help',\n",
       " 'hes',\n",
       " 'high',\n",
       " 'hline',\n",
       " 'home',\n",
       " 'hours',\n",
       " 'housework',\n",
       " 'hstep',\n",
       " 'humans',\n",
       " 'hurts',\n",
       " 'hx',\n",
       " 'id',\n",
       " 'ifstruser',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'image',\n",
       " 'important',\n",
       " 'include',\n",
       " 'increase',\n",
       " 'indium',\n",
       " 'information',\n",
       " 'input',\n",
       " 'insignificant',\n",
       " 'instead',\n",
       " 'int',\n",
       " 'integers',\n",
       " 'interested',\n",
       " 'islands',\n",
       " 'issue',\n",
       " 'ive',\n",
       " 'japanese',\n",
       " 'just',\n",
       " 'key',\n",
       " 'kids',\n",
       " 'killed',\n",
       " 'kind',\n",
       " 'kinetic',\n",
       " 'know',\n",
       " 'large',\n",
       " 'later',\n",
       " 'leaded',\n",
       " 'leg',\n",
       " 'legs',\n",
       " 'lemma',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'level',\n",
       " 'levels',\n",
       " 'lgbt',\n",
       " 'life',\n",
       " 'like',\n",
       " 'limb',\n",
       " 'linked',\n",
       " 'linux',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'live',\n",
       " 'living',\n",
       " 'llama',\n",
       " 'loan',\n",
       " 'loans',\n",
       " 'location',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lost',\n",
       " 'loudest',\n",
       " 'love',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'lsb',\n",
       " 'macarthur',\n",
       " 'machine',\n",
       " 'machines',\n",
       " 'main',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'management',\n",
       " 'manifolds',\n",
       " 'match',\n",
       " 'math',\n",
       " 'mathematical',\n",
       " 'mathematically',\n",
       " 'mathematics',\n",
       " 'mathequation',\n",
       " 'matte',\n",
       " 'maxboundarycellmeasure',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'mechanical',\n",
       " 'mechanics',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'member',\n",
       " 'men',\n",
       " 'mesh',\n",
       " 'mi',\n",
       " 'milan',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'moving',\n",
       " 'muscle',\n",
       " 'muscles',\n",
       " 'nail',\n",
       " 'naturally',\n",
       " 'near',\n",
       " 'nearest',\n",
       " 'need',\n",
       " 'nested',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'node',\n",
       " 'nonorientable',\n",
       " 'normal',\n",
       " 'note',\n",
       " 'nuclear',\n",
       " 'number',\n",
       " 'oa',\n",
       " 'object',\n",
       " 'obviously',\n",
       " 'occasionally',\n",
       " 'offer',\n",
       " 'office',\n",
       " 'old',\n",
       " 'ones',\n",
       " 'open',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orcs',\n",
       " 'order',\n",
       " 'origin',\n",
       " 'original',\n",
       " 'output',\n",
       " 'paper',\n",
       " 'parent',\n",
       " 'particular',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'pause',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'pc',\n",
       " 'people',\n",
       " 'person',\n",
       " 'philippines',\n",
       " 'phrase',\n",
       " 'pi',\n",
       " 'place',\n",
       " 'plane',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'plotrange',\n",
       " 'plotted',\n",
       " 'point',\n",
       " 'points',\n",
       " 'political',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'potential',\n",
       " 'present',\n",
       " 'pretty',\n",
       " 'prevent',\n",
       " 'print',\n",
       " 'problem',\n",
       " 'process',\n",
       " 'projective',\n",
       " 'proof',\n",
       " 'provide',\n",
       " 'public',\n",
       " 'push',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'random',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'regionunion',\n",
       " 'related',\n",
       " 'relationship',\n",
       " 'remove',\n",
       " 'required',\n",
       " 'resolution',\n",
       " 'result',\n",
       " 'return',\n",
       " 'right',\n",
       " 'righttoleft',\n",
       " 'roughly',\n",
       " 'round',\n",
       " 'rule',\n",
       " 'run',\n",
       " 'running',\n",
       " 'russian',\n",
       " 'said',\n",
       " 'sales',\n",
       " 'satisfy',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'second',\n",
       " 'secure',\n",
       " 'seen',\n",
       " 'select',\n",
       " 'service',\n",
       " 'set',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'shes',\n",
       " 'shoes',\n",
       " 'short',\n",
       " 'shortest',\n",
       " 'shortly',\n",
       " 'showmeshwireframe',\n",
       " 'shut',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'single',\n",
       " 'situation',\n",
       " 'situations',\n",
       " 'sleep',\n",
       " 'slightly',\n",
       " 'slowly',\n",
       " 'snpb',\n",
       " 'solder',\n",
       " 'solution',\n",
       " 'son',\n",
       " 'sound',\n",
       " 'spaghetti',\n",
       " 'special',\n",
       " 'spend',\n",
       " 'square',\n",
       " 'stairs',\n",
       " 'start',\n",
       " 'started',\n",
       " 'state',\n",
       " 'stay',\n",
       " 'step',\n",
       " 'steps',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'student',\n",
       " 'stuff',\n",
       " 'sure',\n",
       " 'takes',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'task',\n",
       " 'te',\n",
       " 'team',\n",
       " 'tell',\n",
       " 'terms',\n",
       " 'test',\n",
       " 'tex',\n",
       " 'text',\n",
       " 'thanks',\n",
       " 'theres',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'told',\n",
       " 'totally',\n",
       " 'training',\n",
       " 'tried',\n",
       " 'true',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'understand',\n",
       " 'unsure',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'valueoption',\n",
       " 'values',\n",
       " 'var',\n",
       " 'velocity',\n",
       " 'version',\n",
       " 'virtual',\n",
       " 'walking',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'watched',\n",
       " 'way',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'wife',\n",
       " 'windows',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'wouldnt',\n",
       " 'wrong',\n",
       " 'year',\n",
       " 'young',\n",
       " 'zero',\n",
       " 'zombies',\n",
       " 'тормоз']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
